{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5: n-grams, collocations, TF-IDF\n",
    "\n",
    "For most this problem set, you'll be repurposing [the code from class 9.2 demonstrating how to use the nltk library to get n-grams and collocations](https://github.com/CSCI-2349-F23/sample_code/blob/main/class9.2/Class_9.2_ngrams.ipynb). You'll need to do a little of your own work for reading in files, writing functions, storing data properly, but nearly all the code you need is in the Class9.2 notebook. \n",
    "\n",
    "At the end of the problem set, you will paste in the TF-IDF code from the [sample code for Class 13.1](https://github.com/CSCI-2349-F23/sample_code/blob/main/class13.1/tfidf.ipynb) and edit it so that it works with your data.\n",
    "\n",
    "Instructions for how to submit are at the end of the README. \n",
    "\n",
    "Both PS5 and PS6 are due Monday, November 27, at 11:59pm EST.\n",
    "\n",
    "\n",
    "## Part 0: Install the necessary libraries.\n",
    "\n",
    "You should have most of these installed already, but you can double check. The last two or three might be new for you.\n",
    "\n",
    "```\n",
    "python3 -m pip install jupyter\n",
    "python3 -m pip install nltk\n",
    "python3 -m pip install numpy\n",
    "python3 -m pip install matplotlib\n",
    "python3 -m pip install scikit-learn\n",
    "python3 -m pip install pandas\n",
    "python3 -m pip install altair\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Make sure you have the data\n",
    "\n",
    "If you have not done so already, please follow the directions from the REAMDE to get your data. You should have a directory corresponding to one of your Wikipedia category pages, which contains one text file per Wikipedia page in that category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Read in and tokenize the text files\n",
    "\n",
    "The comments in the code block below will tell you what you need to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import re\n",
    "import nltk\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the nlkt stopword list and improve it, as in prior problem sets.\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist.extend([\")\", \"(\", \".\", \",\", \"?\", \"could\", \"would\", \"“\", \"”\", \"’\", \";\", \"!\",\"much\", \"like\", \"one\", \"many\", \"though\", \"without\", \"upon\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that reads the contents of a directory of\n",
    "# Wikipedia text files into a list of lists of tokens,\n",
    "# with one list per txt file.\n",
    "# Argument: a string that is the name of the directory\n",
    "# Returns: a list of lists of tokens, one per text file\n",
    "\n",
    "def read_and_tokenize(directory_name):\n",
    "\n",
    "    # file in your code here!\n",
    "    \n",
    "    return listoftokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function on your directory\n",
    "# and save what it returns as pagelist\n",
    "\n",
    "# Example I would use for my Wiki category\n",
    "# pagelist = read_and_tokenize(\"Endangered_animals\")\n",
    "\n",
    "# Write you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Bigrams\n",
    "\n",
    "Now you will count and print out bigrams in each of the lists of tokens. Make sure to print out the bigrams so that they look nice and not like Python tuples or lists. Here's an example of some output for my data. This is nice human-readable text. Try to make your output look like this!\n",
    "\n",
    "```\n",
    "Endangered_animals/Wattled_curassow.txt\n",
    "** Most frequent bigrams **\n",
    ". The\n",
    "of the\n",
    ", and\n",
    "wattled curassow\n",
    "in the\n",
    ", the\n",
    ", but\n",
    "curassow (\n",
    ". It\n",
    "the wattled\n",
    "\n",
    "** Most frequent bigrams with no stop words **\n",
    "wattled curassow\n",
    "red-billed curassow\n",
    "black curassow\n",
    "C. globulosa\n",
    "black plumage\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that counts the bigrams for a string\n",
    "# of tokens and then does the following:\n",
    "# (1) prints out the 10 most frequent bigrams.\n",
    "# (2) all bigrams in the top 50 where neither token is a stop word\n",
    "# Argument: a list of tokens\n",
    "\n",
    "\n",
    "def print_common_bigrams(tokenlist):\n",
    "\n",
    "    # fill in your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call your function on each token list in your list of lists\n",
    "# from the Wikipedia directory but limit yourself to pages where \n",
    "# here the number of tokens is greater than 1000.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Collocations\n",
    "\n",
    "Now you are going to print out the most common collocations for each list of tokens. Use **PMI** to rank your collocations, and use a **frequency filter of 2**. Start with collocations where the **two words are adjacent** (the default behavior for collocations in nltk). \n",
    "\n",
    "As above, this code is all available in the [sample code for Class 9.2](https://github.com/CSCI-2349-F23/sample_code/blob/main/class9.2/Class_9.2_ngrams.ipynb).\n",
    "\n",
    "**Continue to make the output look nice!** Here is an example of my output:\n",
    "\n",
    "```\n",
    "Endangered_animals/Wattled_curassow.txt\n",
    "** Common Collocations **\n",
    "Development Reserve\n",
    "Mamirauá Sustainable\n",
    "Sustainable Development\n",
    "von Spix\n",
    "A captive\n",
    "well studied\n",
    "a. alector\n",
    "Adult male\n",
    "ancient lineage\n",
    "habitat destruction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statement\n",
    "from nltk.collocations import *\n",
    "\n",
    "# Create the object you need to get collocations.\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Write a function that prints out the top 10 collocations in\n",
    "# a list of tokens using PMI for the ranking metric and \n",
    "# a frequency filter of 2.\n",
    "# Argument: a list of tokens\n",
    "\n",
    "def print_collocations(tokenlist):\n",
    "\n",
    "    # put your code here!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call your function on each token list in your list of lists\n",
    "# from the first directory where the number of tokens is greater\n",
    "# than 1000.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: TF-IDF\n",
    "\n",
    "TF-IDF stands for \"term frequency - inverse document frequency\". It's a way of measuring how common a word is in a document (\"term frequency\") relative to how common that word is in all your documents (\"inverse document frequency\"). This metric allows you to discover the words that are central to a particular document and make that document special or unique compared to other documents. \n",
    "\n",
    "For example, in my endangered animals example, all of the documents (of course) contain the same stop words, but they are also likely to contain many of the same content words -- *animal, endangered, threated, species*. Applying TF-IDF will allow the words that are particularly important for a particular document to be highlighted. \n",
    "\n",
    "This technique will be useful to your in your projects, when you try to highlight for your audience how different examples of the same kind of document (e.g., song lyrics, earnings calls, literature) are different from one another (e.g., over time, produced by a different company, written in a different genre).\n",
    "\n",
    "In the sample code for class 13.1, you will find a notebook and my directory of Wikipedia pages for the Endangered Animals category. Run that code with that dataset so you can see how it works. Then come back here, and paste it in so that it works with your dataset. Of course you will have to change anything that has been hard-coded (e.g., how I look up information about the term \"bird\" or the document \"African_bush_elephant\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paste your code here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Have several blocks, each with a few related commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will ensure that you know exactly where each error or bug is coming from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Complete this jupyter notebook. This will be your submission for PS5. Instructions for how to submit are provided in the README."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
